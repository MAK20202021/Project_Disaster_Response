{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "df = pd.read_sql_table('Response_table', 'sqlite:///Disaster_Response.db')  \n",
    "X = df.message.values\n",
    "Y = df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'message', 'original', 'genre', 'related', 'request', 'offer',\n",
       "       'aid_related', 'medical_help', 'medical_products', 'search_and_rescue',\n",
       "       'security', 'military', 'child_alone', 'water', 'food', 'shelter',\n",
       "       'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather', 'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "related                   0\n",
       "request                   0\n",
       "offer                     0\n",
       "aid_related               0\n",
       "medical_help              0\n",
       "medical_products          0\n",
       "search_and_rescue         0\n",
       "security                  0\n",
       "military                  0\n",
       "child_alone               0\n",
       "water                     0\n",
       "food                      0\n",
       "shelter                   0\n",
       "clothing                  0\n",
       "money                     0\n",
       "missing_people            0\n",
       "refugees                  0\n",
       "death                     0\n",
       "other_aid                 0\n",
       "infrastructure_related    0\n",
       "transport                 0\n",
       "buildings                 0\n",
       "electricity               0\n",
       "tools                     0\n",
       "hospitals                 0\n",
       "shops                     0\n",
       "aid_centers               0\n",
       "other_infrastructure      0\n",
       "weather_related           0\n",
       "floods                    0\n",
       "storm                     0\n",
       "fire                      0\n",
       "earthquake                0\n",
       "cold                      0\n",
       "other_weather             0\n",
       "direct_report             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26042, 36)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather update - a cold front from Cuba that could pass over Haiti\n",
      "['weather', 'update', '-', 'a', 'cold', 'front', 'from', 'cuba', 'that', 'could', 'pas', 'over', 'haiti'] \n",
      "\n",
      "Is the Hurricane over or is it not over\n",
      "['is', 'the', 'hurricane', 'over', 'or', 'is', 'it', 'not', 'over'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in X[:2]:\n",
    "    tokens = tokenize(message)\n",
    "    print(message)\n",
    "    print(tokens,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "pipeline = Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer()),('clf',MultiOutputClassifier(RandomForestClassifier()))])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.33, random_state=42)\n",
    "#Train pipeline\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.37      0.05      0.09      1495\n",
      "               request       0.00      0.00      0.00        46\n",
      "                 offer       0.43      0.18      0.25      3563\n",
      "           aid_related       0.11      0.01      0.01       702\n",
      "          medical_help       0.03      0.00      0.00       425\n",
      "      medical_products       0.00      0.00      0.00       245\n",
      "     search_and_rescue       0.00      0.00      0.00       160\n",
      "              security       0.00      0.00      0.00       272\n",
      "              military       0.00      0.00      0.00         0\n",
      "           child_alone       0.16      0.01      0.01       555\n",
      "                 water       0.22      0.01      0.02       989\n",
      "                  food       0.16      0.01      0.02       752\n",
      "               shelter       0.00      0.00      0.00       140\n",
      "              clothing       0.00      0.00      0.00       208\n",
      "                 money       0.00      0.00      0.00        97\n",
      "        missing_people       0.10      0.00      0.01       299\n",
      "              refugees       0.11      0.00      0.00       397\n",
      "                 death       0.14      0.01      0.02      1101\n",
      "             other_aid       0.15      0.01      0.01       534\n",
      "infrastructure_related       0.08      0.01      0.01       384\n",
      "             transport       0.08      0.00      0.01       449\n",
      "             buildings       0.00      0.00      0.00       179\n",
      "           electricity       0.00      0.00      0.00        55\n",
      "                 tools       0.00      0.00      0.00        94\n",
      "             hospitals       0.00      0.00      0.00        44\n",
      "                 shops       0.00      0.00      0.00        96\n",
      "           aid_centers       0.06      0.00      0.01       356\n",
      "  other_infrastructure       0.54      0.15      0.23      2402\n",
      "       weather_related       0.25      0.00      0.01       722\n",
      "                floods       0.36      0.03      0.06       789\n",
      "                 storm       0.00      0.00      0.00       100\n",
      "                  fire       0.66      0.13      0.22       816\n",
      "            earthquake       0.00      0.00      0.00       192\n",
      "                  cold       0.20      0.00      0.01       459\n",
      "         other_weather       0.39      0.05      0.09      1684\n",
      "\n",
      "           avg / total       0.29      0.06      0.10     20801\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 35, does not match size of target_names, 36\n",
      "  .format(len(labels), len(target_names))\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "m = Y.columns\n",
    "print(classification_report(y_test.iloc[:,1:].values, np.array([x[1:] for x in y_pred]), target_names=m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy                   index\n",
      "0   0.729579                 related\n",
      "1   0.819525                 request\n",
      "2   0.994415                   offer\n",
      "3   0.562253             aid_related\n",
      "4   0.914242            medical_help\n",
      "5   0.946940        medical_products\n",
      "6   0.970212       search_and_rescue\n",
      "7   0.980684                security\n",
      "8   0.967885                military\n",
      "9   1.000000             child_alone\n",
      "10  0.933442                   water\n",
      "11  0.881894                    food\n",
      "12  0.909006                 shelter\n",
      "13  0.983593                clothing\n",
      "14  0.975448                   money\n",
      "15  0.988597          missing_people\n",
      "16  0.964277                refugees\n",
      "17  0.952990                   death\n",
      "18  0.863859               other_aid\n",
      "19  0.936235  infrastructure_related\n",
      "20  0.952990               transport\n",
      "21  0.945194               buildings\n",
      "22  0.979055             electricity\n",
      "23  0.993600                   tools\n",
      "24  0.988946               hospitals\n",
      "25  0.994880                   shops\n",
      "26  0.988597             aid_centers\n",
      "27  0.956830    other_infrastructure\n",
      "28  0.726088         weather_related\n",
      "29  0.915290                  floods\n",
      "30  0.905632                   storm\n",
      "31  0.988248                    fire\n",
      "32  0.911101              earthquake\n",
      "33  0.977542                    cold\n",
      "34  0.945892           other_weather\n",
      "35  0.798464           direct_report\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "target_names = Y.columns\n",
    "y_true = np.array(y_test)\n",
    "accuracy = pd.DataFrame()\n",
    "for i,target in enumerate(target_names):\n",
    "    Accuracy_Score = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    accuracy = accuracy.append({'Accuracy':Accuracy_Score,'index':target},ignore_index = True)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the parameters in the pipeline\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'tfidf__use_idf':[True, False],\n",
    "              'clf__estimator__min_samples_leaf': [1, 2],\n",
    "              'clf__estimator__min_samples_split': [2 , 3]\n",
    "              }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tunned = cv.fit(X_train,y_train)\n",
    "y_pred = model_tunned.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "m = Y.columns\n",
    "print(classification_report(y_test.iloc[:,1:].values, np.array([x[1:] for x in y_pred]), target_names=m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy                   index\n",
      "0   0.732721                 related\n",
      "1   0.820922                 request\n",
      "2   0.994647                   offer\n",
      "3   0.570980             aid_related\n",
      "4   0.915988            medical_help\n",
      "5   0.948103        medical_products\n",
      "6   0.970910       search_and_rescue\n",
      "7   0.981266                security\n",
      "8   0.968001                military\n",
      "9   1.000000             child_alone\n",
      "10  0.933209                   water\n",
      "11  0.881545                    food\n",
      "12  0.909821                 shelter\n",
      "13  0.983477                clothing\n",
      "14  0.975797                   money\n",
      "15  0.988597          missing_people\n",
      "16  0.964626                refugees\n",
      "17  0.952758                   death\n",
      "18  0.864208               other_aid\n",
      "19  0.935653  infrastructure_related\n",
      "20  0.953805               transport\n",
      "21  0.945078               buildings\n",
      "22  0.978590             electricity\n",
      "23  0.993600                   tools\n",
      "24  0.988713               hospitals\n",
      "25  0.994880                   shops\n",
      "26  0.988713             aid_centers\n",
      "27  0.957761    other_infrastructure\n",
      "28  0.726902         weather_related\n",
      "29  0.914708                  floods\n",
      "30  0.905050                   storm\n",
      "31  0.988131                    fire\n",
      "32  0.910054              earthquake\n",
      "33  0.977426                    cold\n",
      "34  0.944613           other_weather\n",
      "35  0.799162           direct_report\n"
     ]
    }
   ],
   "source": [
    "target_names = Y.columns\n",
    "y_true = np.array(y_test)\n",
    "accuracy = pd.DataFrame()\n",
    "for i,target in enumerate(target_names):\n",
    "    Accuracy_Score = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    accuracy = accuracy.append({'Accuracy':Accuracy_Score,'index':target},ignore_index = True)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pipeline = Pipeline([('vect',CountVectorizer()),('tfidf',TfidfTransformer()),('clf',MultiOutputClassifier(KNeighborsClassifier()))])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "              metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "              weights='uniform'),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': None,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'auto',\n",
       " 'clf__estimator__leaf_size': 30,\n",
       " 'clf__estimator__metric': 'minkowski',\n",
       " 'clf__estimator__metric_params': None,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__n_neighbors': 5,\n",
       " 'clf__estimator__p': 2,\n",
       " 'clf__estimator__weights': 'uniform',\n",
       " 'clf__estimator': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters= {'tfidf__use_idf':[True, False],\n",
    "             'clf__estimator__n_neighbors': [5,10]\n",
    "            }\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model again\n",
    "Model_KN = cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Model_KN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Y.columns\n",
    "print(classification_report(y_test.iloc[:,1:].values, np.array([x[1:] for x in y_pred]), target_names=m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the accuracy:\n",
    "target_names = Y.columns\n",
    "y_true = np.array(y_test)\n",
    "accuracy = pd.DataFrame()\n",
    "for i,target in enumerate(target_names):\n",
    "    Accuracy_Score = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    accuracy = accuracy.append({'Accuracy':Accuracy_Score,'index':target},ignore_index = True)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_tunned, open('classifier.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
